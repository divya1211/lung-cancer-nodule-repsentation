{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import datasets as mt_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd, optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/scratch/dj1311/Lung_cancer/LIDC_conversion5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtensor = torch.load('nodules.pt')\n",
    "malignancy = torch.load('malignancies.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malignancy.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load data\n",
    "train_dataset = mt_datasets.TensorDataset(newtensor, malignancy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50, 50])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(train_dataset, batch_size=10,num_workers=2)\n",
    "\n",
    "# for i in loader:\n",
    "#     print((i[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50, 50])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i[0].mean(dim=-1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-6e3b76e446eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.cat([i[0].mean(dim=-1).unsqueeze(1), i[0].mean(dim=-2).unsqueeze(1), i[0].mean(dim=-3).unsqueeze(1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.6563e-01, -2.9549e-01, -3.7718e-01, -1.6637e+00, -7.2040e-01,\n",
       "         -5.8861e-01, -7.6297e-01,  4.9152e-01,  2.4781e-01, -8.8927e-01,\n",
       "         -1.0780e+00, -1.0726e+00, -6.2249e-01, -1.0416e+00, -1.3725e+00,\n",
       "         -3.8951e-01, -1.0661e+00, -2.7430e-01, -6.3206e-01, -3.8163e-01,\n",
       "         -1.5493e+00, -9.8003e-01, -1.3047e+00,  2.5629e-02, -7.7691e-01,\n",
       "         -1.2585e+00, -9.2962e-01, -1.1010e+00, -9.9180e-01, -4.3211e-01,\n",
       "         -6.7484e-01, -9.5563e-01, -4.4542e-01, -5.6406e-01, -5.3343e-01,\n",
       "         -5.9879e-01,  4.6881e-01, -9.1018e-01, -8.1560e-01, -6.9065e-03,\n",
       "         -1.2254e+00, -9.0704e-01, -1.2914e+00, -2.6382e-01, -6.6361e-01,\n",
       "         -5.7536e-01, -1.1660e+00, -8.9300e-01, -1.2992e+00, -1.1147e+00,\n",
       "         -2.6103e-01,  3.6596e-01, -2.3121e-01, -4.7829e-01, -1.1834e-02,\n",
       "         -1.2269e+00, -2.4062e-01, -1.3164e+00, -1.4446e-01, -6.9826e-01,\n",
       "          6.1819e-01,  1.3015e-01,  7.0768e-02,  4.1377e-03, -1.0448e+00,\n",
       "         -2.5598e-01, -2.9994e-01, -1.7746e-01, -7.3125e-01, -9.7369e-01,\n",
       "         -1.0904e+00,  3.4887e-01, -1.5170e+00, -2.3646e-01, -1.2331e+00,\n",
       "         -1.3022e+00,  1.5067e-01, -3.9381e-01,  4.9613e-01,  4.4172e-01,\n",
       "         -9.5491e-01, -1.6682e+00, -8.7292e-02, -9.0942e-01, -7.1943e-01,\n",
       "         -3.6654e-01, -2.7721e-02,  1.4875e-01, -1.1307e-01, -8.8572e-01,\n",
       "         -1.5491e+00, -1.3499e+00, -2.1478e+00, -3.7273e-01,  3.1956e-02,\n",
       "         -2.3339e+00, -7.5299e-01, -5.2193e-01, -1.6171e+00, -2.9382e-01,\n",
       "         -9.3740e-01, -1.1224e+00, -9.2649e-01, -5.6291e-02, -2.3199e-01,\n",
       "         -6.1654e-01, -4.4623e-01, -1.2587e+00, -8.2527e-01, -1.7582e+00,\n",
       "         -1.3893e+00, -3.6059e-01,  1.0694e+00,  9.4192e-02,  3.1132e-01,\n",
       "         -1.1819e+00, -6.2960e-01, -2.1441e-01,  6.9478e-01, -2.0957e-01,\n",
       "         -4.3582e-01,  3.0005e-01,  8.4995e-01,  6.1276e-01,  1.5271e+00,\n",
       "         -1.0319e-01,  7.5957e-01, -1.3351e+00, -1.2735e+00, -9.8859e-01,\n",
       "         -1.1827e+00, -1.3786e+00, -6.7090e-01, -1.4073e+00, -4.3667e-01,\n",
       "         -1.2969e+00, -1.0965e+00, -1.2253e+00, -1.5480e+00, -1.5793e+00,\n",
       "         -1.6778e+00, -1.7719e+00, -2.1555e+00, -1.8777e+00, -4.2031e-01,\n",
       "         -3.4446e-01, -9.2511e-01, -1.6587e+00, -9.8914e-01, -1.6344e+00,\n",
       "          6.6879e-01,  1.7045e+00, -1.0594e+00, -4.5001e-01,  8.1256e-02,\n",
       "          2.5229e-01, -5.2098e-01, -1.6237e-01,  4.4475e-01,  1.1990e-01,\n",
       "          5.4099e-01,  6.5767e-01,  2.0467e-01,  6.4677e-01,  3.6972e-01,\n",
       "          1.4345e-02, -2.3851e-01, -3.8408e-01,  6.4022e-01, -4.2697e-01,\n",
       "         -3.3032e-02,  7.6807e-01,  4.4060e-01,  2.1061e-01,  3.0582e-01,\n",
       "         -6.1106e-01,  1.2546e-01,  2.2329e-01,  4.5917e-01,  6.9168e-01,\n",
       "          5.6271e-01,  1.7225e-01,  5.7317e-01,  1.0111e-01,  6.1742e-01,\n",
       "          4.9487e-01,  4.4969e-01,  5.7659e-01,  9.2774e-02,  4.5246e-01,\n",
       "         -4.7356e-01,  2.8652e-01,  5.6939e-01,  6.1452e-01, -3.4665e-01,\n",
       "          7.5189e-01,  2.5752e-01,  4.0319e-01,  3.0781e-01,  5.2944e-01,\n",
       "          1.9824e-01,  6.2520e-01,  6.9232e-01,  3.6447e-01,  3.1289e-01,\n",
       "         -6.0339e-02, -3.4228e-01,  1.9910e-01,  9.9384e-01,  3.0220e-01,\n",
       "         -2.0005e-01,  1.9481e-01,  4.4452e-01, -2.0752e-01,  3.2943e-02,\n",
       "         -1.4813e-01, -2.0143e-01,  2.0814e-01, -4.8008e-01,  6.0308e-01,\n",
       "          1.7140e-01, -1.4685e-01, -1.4429e-01,  7.6941e-01,  7.6325e-02,\n",
       "          4.8362e-01,  2.1759e-01,  7.1242e-01, -3.2636e-01, -1.6160e-01,\n",
       "         -1.0536e-02,  2.2544e-01,  3.8246e-01,  1.7330e-01,  7.5589e-01,\n",
       "          6.9718e-01,  3.8576e-01,  5.4473e-01,  7.0012e-01, -1.0290e-01,\n",
       "          5.2235e-01,  5.7662e-02,  5.2329e-01,  2.8732e-01, -5.8367e-02,\n",
       "          5.4763e-01,  3.7628e-01, -3.8155e-02,  7.2945e-01,  1.8023e-01,\n",
       "          4.7180e-01,  4.6315e-01, -6.6685e-01,  6.4326e-01,  9.1524e-01,\n",
       "         -5.5660e-01,  3.5438e-01,  1.2181e-01, -4.9323e-01,  9.4187e-02,\n",
       "         -4.2806e-01, -7.2668e-01, -9.9886e-02,  2.9365e-01,  6.6680e-01,\n",
       "          7.1094e-01,  2.3782e-01,  6.0448e-01, -8.7657e-02, -2.6003e-01,\n",
       "         -9.2220e-01, -1.0716e+00, -3.8116e-01,  8.5830e-01, -1.0854e+00,\n",
       "         -1.2082e+00, -1.1339e+00, -6.9440e-01, -1.1248e+00, -6.6028e-01,\n",
       "         -7.5357e-02,  8.9913e-01,  8.8001e-01, -2.2707e-01,  3.0499e-01,\n",
       "          7.8133e-01, -2.7526e-01,  7.0884e-02, -7.8348e-01, -1.5779e+00,\n",
       "         -9.6062e-01, -1.3079e+00, -3.1192e-01, -1.3737e+00, -1.0675e+00,\n",
       "         -9.5500e-01, -1.0471e+00, -1.4479e+00, -6.9332e-01, -2.9182e-01,\n",
       "         -1.7108e+00, -7.1984e-01, -2.9888e-01,  4.1945e-02, -1.0583e+00,\n",
       "         -1.0176e+00,  2.1976e-01, -8.2657e-01, -1.0556e+00, -6.9671e-02,\n",
       "          6.0031e-01, -1.1683e-01,  1.0838e-02,  1.2274e-01,  1.1427e+00,\n",
       "         -4.2167e-01, -4.8778e-01, -8.9029e-01, -1.1678e+00, -7.0057e-01,\n",
       "         -1.4003e+00, -1.2182e+00, -1.5603e+00, -1.9198e+00, -1.6636e+00,\n",
       "         -1.8225e+00, -1.6399e+00, -1.5553e-01, -3.7719e-01, -3.8568e-01,\n",
       "         -3.4652e-02, -3.8813e-01, -1.8179e-01,  2.1745e-02, -6.4593e-01,\n",
       "         -7.0407e-01, -1.6880e+00,  1.9746e-01,  4.9434e-01, -1.0185e+00,\n",
       "         -5.1569e-01,  5.5910e-01, -4.9014e-01, -1.3258e+00, -1.0559e+00,\n",
       "          8.0546e-01, -8.4193e-01, -1.6174e+00,  1.2013e-01, -1.1019e+00,\n",
       "         -1.0689e+00, -2.2622e+00, -1.5602e+00, -8.1344e-01, -9.1664e-01,\n",
       "          3.4604e-01,  1.0252e+00, -1.7613e-01,  2.3822e-01,  2.7849e-01,\n",
       "          1.9031e-01,  4.3651e-01, -9.7749e-02,  5.1922e-02, -2.8475e-01,\n",
       "         -6.4488e-01, -9.3042e-01, -5.9348e-01, -7.0812e-01, -6.0497e-01,\n",
       "         -5.9860e-01, -4.1897e-01, -7.0646e-01, -1.8059e-01, -2.9980e-01,\n",
       "         -9.0040e-01, -1.1593e+00,  1.8920e-01, -3.4127e-01, -3.6842e-01,\n",
       "          3.2295e-01, -2.5382e-01, -9.7493e-02, -6.0531e-01, -7.8431e-01,\n",
       "         -7.5203e-01, -1.2800e+00, -9.4904e-01, -9.2708e-01, -2.0054e-01,\n",
       "          5.6501e-01,  2.1930e-01, -1.7376e+00, -1.6327e+00, -3.4233e-01,\n",
       "          6.0129e-01, -9.5098e-01, -9.5472e-01,  3.8122e-01, -1.5405e-01,\n",
       "         -9.1816e-01,  9.3056e-01,  5.0857e-01, -2.0822e+00, -1.5495e+00,\n",
       "         -5.4090e-01, -1.0321e-02, -4.0545e-01, -3.0530e-01,  1.1356e+00,\n",
       "         -2.2308e-01,  3.9021e-01,  1.8881e+00,  5.7450e-01,  5.9840e-01,\n",
       "          7.6272e-01, -1.6348e-01,  3.3652e-01,  5.3654e-01,  1.0373e+00,\n",
       "          9.0728e-01,  1.5082e+00, -8.4370e-03,  4.0216e-01,  2.4993e-01,\n",
       "         -7.0772e-01,  2.4716e-01,  1.2462e+00,  1.9874e+00,  2.7868e-01,\n",
       "         -8.9576e-01,  9.2681e-03,  5.8978e-01,  5.4490e-01,  3.9510e-01,\n",
       "          1.0510e+00, -2.7941e-01, -4.8798e-01,  3.9028e-01,  3.9713e-01,\n",
       "          8.6528e-01,  4.7383e-01,  3.1010e-01, -5.7648e-01, -5.7537e-01,\n",
       "          1.9593e-01,  2.3787e-01,  1.5358e+00,  9.9051e-01, -3.9930e-01,\n",
       "          1.7384e-01,  9.0491e-01,  3.2729e-01, -3.6468e-01, -3.5367e-01,\n",
       "          4.3018e-01,  1.7422e+00,  1.1074e+00, -1.9910e-01,  6.1847e-01,\n",
       "         -9.3883e-01,  8.4853e-01,  8.8675e-01,  2.4842e+00,  1.0628e+00,\n",
       "         -2.5164e-01, -9.6243e-01, -1.5260e-01,  2.2755e-04,  1.3268e+00,\n",
       "          1.1087e+00,  5.3023e-01,  4.3493e-01,  8.8879e-01, -2.8277e-01,\n",
       "         -1.4342e-01,  3.3250e-01,  3.7771e-01,  4.0455e-01,  5.3652e-01,\n",
       "         -1.2321e-01,  5.5372e-01,  8.5929e-01, -9.5298e-01, -1.2071e+00,\n",
       "          1.9791e-01, -6.1467e-02,  1.4198e+00,  1.5638e+00,  1.2022e+00,\n",
       "          2.1128e-01,  7.3986e-01,  6.4955e-01, -1.2868e+00,  1.2552e+00,\n",
       "         -8.5542e-01,  8.3026e-02, -1.1462e-01, -1.3829e-01,  1.2176e+00,\n",
       "         -1.8340e+00,  5.3045e-01,  1.2367e+00,  8.9935e-01,  8.1725e-01,\n",
       "          1.4298e+00,  1.3388e+00,  8.7467e-01,  5.4284e-01,  4.2598e-01,\n",
       "         -9.3883e-01, -8.7626e-01,  7.0425e-01,  8.5738e-01,  1.1433e+00,\n",
       "          1.7904e+00,  6.4954e-01,  4.2575e-02,  1.2572e+00,  5.1626e-01,\n",
       "         -8.4030e-01,  3.5439e-01,  7.8601e-01,  1.5387e+00,  7.1985e-01,\n",
       "         -6.9983e-01, -2.8083e-01, -5.7755e-01,  3.4225e-01, -2.3164e-01,\n",
       "          1.0885e+00,  4.2011e-01,  2.0085e-01, -1.0445e+00,  6.2524e-01,\n",
       "         -2.5715e-01, -3.9863e-01, -2.0033e-01,  3.1455e-01,  1.1321e+00,\n",
       "         -9.2411e-01,  1.6343e+00,  1.1460e+00,  7.9330e-01,  3.7044e-01,\n",
       "          1.1200e+00,  7.6399e-01, -1.7663e+00, -1.2287e+00, -3.8669e-01,\n",
       "          4.3463e-02, -1.5496e-01,  7.9674e-01, -5.3248e-01, -1.3516e+00,\n",
       "         -6.4993e-01,  1.0987e-01,  3.7579e-01,  1.5365e+00,  7.7215e-01,\n",
       "          1.6163e-01, -3.8462e-01,  7.8824e-01,  1.3770e-01, -1.3266e+00,\n",
       "         -6.9679e-01,  6.9227e-01,  1.2061e+00,  3.9356e-01, -4.4671e-01,\n",
       "          1.2204e+00,  1.2537e-01,  8.9048e-01, -5.2710e-01,  1.3246e-01,\n",
       "         -4.7713e-01, -5.7462e-01,  1.0928e+00,  5.0552e-01,  2.7518e-01,\n",
       "          3.1090e-01, -6.3584e-02,  4.9220e-01,  5.5281e-01,  8.9105e-01,\n",
       "          8.5715e-01, -5.5220e-01,  1.3781e+00,  7.4561e-01,  1.0053e+00,\n",
       "         -4.0068e-01,  3.1239e-01, -6.5007e-01,  1.2628e+00,  3.5156e-01,\n",
       "         -7.5992e-01,  9.7926e-01,  3.3240e-02, -4.8848e-01,  8.7368e-01,\n",
       "          2.1500e+00, -1.6039e-01,  8.8874e-02, -3.1160e-01,  6.3515e-01,\n",
       "          4.7228e-01,  1.3114e+00, -5.1855e-01,  6.7437e-01, -4.7112e-02,\n",
       "          9.1066e-01,  5.1072e-01, -3.3224e-01,  6.6879e-01,  1.3396e-01,\n",
       "          2.7334e-01,  1.0882e+00,  4.7071e-01,  1.9129e+00,  7.5063e-01,\n",
       "          1.2808e+00,  7.6089e-01,  1.8350e-01,  5.5244e-01,  2.0864e-01,\n",
       "         -8.8411e-01,  1.1027e+00, -1.0406e-01, -1.1092e+00,  3.5331e-01,\n",
       "          1.5300e-01,  6.5138e-01,  7.6229e-01,  1.0429e+00, -4.6265e-01,\n",
       "          3.1894e-01,  1.3139e+00,  8.4069e-01,  7.8026e-01,  3.7552e-01,\n",
       "         -1.6391e+00,  8.4342e-01, -1.7833e-01,  1.6271e+00,  8.0524e-01,\n",
       "         -6.5107e-01,  6.1991e-01,  5.4922e-01, -1.1312e+00, -1.7945e+00,\n",
       "          1.2441e+00,  1.3366e-01,  4.2356e-01,  8.8516e-01, -2.8591e-01,\n",
       "          6.5008e-01, -2.7467e-01,  6.6694e-02,  3.1170e-01,  4.1165e-01,\n",
       "         -1.5916e-01, -9.5828e-01, -1.4884e-01, -8.5371e-01,  7.3180e-01,\n",
       "          1.0775e-01,  1.3071e+00,  1.9001e-01, -1.0231e+00, -2.5855e-01,\n",
       "          2.8141e-01, -1.8751e-01, -4.4874e-01,  6.9710e-01,  1.3993e+00,\n",
       "         -8.8261e-01,  1.9824e+00,  9.1720e-01,  8.9046e-01,  1.3444e-01,\n",
       "          1.1709e+00,  9.2579e-01, -5.5768e-01,  5.8863e-01,  1.0240e+00,\n",
       "         -1.2471e+00,  1.4369e-01, -7.4807e-01, -3.0128e-01, -9.0503e-01,\n",
       "         -5.5411e-01,  9.8512e-01,  6.3770e-01,  6.7968e-01, -6.2266e-01,\n",
       "          7.8054e-01,  1.7134e+00, -1.8452e-01, -2.9231e-01,  6.8628e-01,\n",
       "          1.7548e+00, -3.3985e-01,  1.2715e-01,  4.4951e-01,  7.1712e-01,\n",
       "         -1.0894e-01, -6.5209e-01,  2.9892e-01,  7.6245e-01,  3.5989e-02,\n",
       "          9.9771e-01,  1.0442e+00, -2.7644e-02, -7.1776e-01,  3.2260e-01,\n",
       "         -1.2813e-01,  5.7269e-01, -7.6484e-01, -3.8192e-01,  5.3723e-01,\n",
       "          4.5737e-01,  3.3706e-02,  1.5552e+00,  4.4650e-01, -3.0400e-01,\n",
       "          1.1791e+00, -5.9233e-01, -2.7242e-01,  1.5026e+00, -5.3052e-01,\n",
       "          1.3344e-01,  2.2117e+00, -4.3995e-01,  2.0939e+00, -1.4473e+00,\n",
       "         -4.8708e-02, -1.9174e-01,  9.1654e-01,  6.5268e-01,  1.5339e-01,\n",
       "          9.4474e-01, -3.5000e-01,  4.6468e-01,  2.1857e-01,  6.7382e-01,\n",
       "          2.7088e-01,  1.9421e-01,  8.4581e-01,  6.9523e-01,  1.4414e+00,\n",
       "          2.6458e-01, -2.9351e-02,  1.1963e-01,  6.3175e-01,  1.1768e+00,\n",
       "         -6.5244e-01,  7.5646e-01, -4.1136e-01,  1.3649e+00,  3.7405e-02,\n",
       "         -1.4257e-01,  6.9964e-01,  4.5688e-01,  9.3297e-01,  1.0920e+00,\n",
       "          8.3988e-01,  6.9092e-01,  6.6718e-01, -2.6904e-01,  1.4287e+00,\n",
       "          7.7713e-01,  2.8932e-01,  1.3886e+00,  8.1131e-01,  8.9850e-01,\n",
       "         -1.4756e-02,  5.3208e-01,  6.3514e-01,  1.2881e+00, -7.0952e-01,\n",
       "         -6.7128e-01, -8.1208e-01,  8.1731e-01,  6.9364e-01,  1.7859e+00,\n",
       "          1.0016e-01,  5.0790e-01,  1.3027e+00,  3.0108e-01, -1.1911e-01,\n",
       "          4.6449e-01,  1.0741e+00,  1.4572e+00,  7.4767e-01,  3.0405e-01,\n",
       "          3.1495e-01,  9.1100e-01,  5.4056e-01, -6.5119e-01,  3.3911e-01,\n",
       "         -5.7363e-01,  1.5350e-01, -9.6881e-01, -9.7228e-01,  8.1774e-01,\n",
       "          1.2813e+00,  3.3731e-01,  1.9009e-01,  1.2848e+00, -3.7652e-02,\n",
       "         -3.6126e-01,  1.3172e+00,  2.0238e-01,  1.6652e+00, -9.0246e-01,\n",
       "         -2.3138e-01,  3.9359e-01, -9.9006e-01,  1.7590e+00,  6.7331e-01,\n",
       "         -1.6508e+00, -9.5912e-01,  4.0033e-01,  6.7435e-01,  7.8072e-01,\n",
       "         -1.2736e+00,  7.7820e-01,  7.2104e-01,  1.2257e+00, -3.1543e-01,\n",
       "          1.3236e+00,  5.1363e-01, -6.2504e-01, -8.6047e-01,  2.7036e-01,\n",
       "          6.5617e-01,  1.7325e+00,  1.5666e+00,  9.1457e-01, -4.8803e-01,\n",
       "          1.5863e+00,  4.7768e-01,  1.4724e-01,  6.3508e-01,  7.4251e-01,\n",
       "          2.0262e+00,  7.2041e-01, -9.3754e-01,  9.0542e-01,  1.0076e+00,\n",
       "          1.3247e+00,  1.5470e+00,  1.7326e+00, -5.4034e-01, -1.0223e-01,\n",
       "          8.6458e-01, -7.7215e-01,  2.4061e-01, -4.0961e-01,  1.3998e+00,\n",
       "          2.6090e-01,  1.2184e+00,  1.2105e+00,  1.5333e-01, -6.6543e-01,\n",
       "          6.6697e-01,  4.1442e-02, -2.6454e-01,  1.4337e+00, -3.6104e-01,\n",
       "          1.0840e+00, -1.4012e+00,  1.2752e+00, -1.1983e+00, -2.2750e+00,\n",
       "          8.7425e-01,  1.4763e+00, -1.8812e-01, -8.8244e-02,  1.5813e+00,\n",
       "          1.0686e+00, -3.1641e-01,  1.3543e+00,  1.2172e+00,  2.7053e-01,\n",
       "          4.8227e-01, -3.4525e-01, -1.4323e-01, -9.4237e-01,  3.8532e-01,\n",
       "         -2.3116e-01,  5.9751e-01,  9.2533e-01, -1.5095e-01, -9.4643e-01,\n",
       "         -4.9967e-01,  1.1012e+00,  9.8334e-01,  2.1856e+00,  1.8839e+00,\n",
       "         -1.3512e+00, -7.2391e-01,  1.6576e+00,  9.3411e-01,  6.6619e-01,\n",
       "         -1.8939e-01, -5.0302e-01,  1.1952e+00, -5.9735e-01,  1.0046e+00,\n",
       "          1.0529e+00,  1.0450e+00,  6.3666e-01, -3.7484e-01, -1.4974e+00,\n",
       "         -5.1623e-01,  9.7876e-02,  4.4336e-01,  7.9624e-01,  7.9552e-02,\n",
       "         -1.5365e-02,  1.0262e+00, -8.8347e-01,  4.7633e-01, -3.7702e-01,\n",
       "         -9.0876e-01, -1.0023e+00, -7.2368e-01, -1.4507e-01,  1.7753e+00,\n",
       "         -3.5484e-01, -2.2791e-01,  3.0110e-01, -1.8101e+00,  2.3953e-01,\n",
       "         -7.1251e-01,  9.9184e-02, -1.4581e-01, -3.9144e-01, -4.5230e-01,\n",
       "         -4.1202e-01, -9.8555e-01, -3.8061e-01,  9.9408e-02, -9.5949e-01,\n",
       "         -8.0274e-01, -1.3075e+00,  1.5982e-03,  4.7328e-01, -4.2079e-01,\n",
       "          1.1508e-01, -4.7229e-01, -1.3174e+00,  1.3045e-01,  7.1738e-01,\n",
       "         -5.2739e-01, -4.4022e-01, -5.7743e-01, -1.1613e-01, -1.1606e+00,\n",
       "          2.5751e-01,  8.1100e-02, -5.7584e-01, -7.7985e-01, -1.3755e+00,\n",
       "         -2.9056e-01,  2.9524e-01, -3.0096e-01,  7.6245e-01,  3.0688e-02,\n",
       "          9.1647e-02,  9.6596e-01, -3.3139e-01, -3.9520e-01, -2.4901e+00,\n",
       "          7.5537e-01, -1.6344e+00,  1.9543e-01,  4.4543e-02, -6.8070e-01,\n",
       "         -7.4131e-01,  3.3637e-02,  7.1843e-01, -5.6406e-01, -8.9930e-01,\n",
       "         -1.2353e+00, -2.5811e+00,  1.2436e+00, -4.4779e-01, -8.9481e-01,\n",
       "         -2.4855e-01, -1.1915e+00, -1.2364e+00, -2.0896e+00, -9.9622e-01,\n",
       "         -5.1307e-01, -1.8042e-02, -8.4988e-01,  1.0826e+00,  1.1606e+00]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet18(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 25, 25]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 25, 25]             128\n",
      "              ReLU-3           [-1, 64, 25, 25]               0\n",
      "         MaxPool2d-4           [-1, 64, 13, 13]               0\n",
      "            Conv2d-5           [-1, 64, 13, 13]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 13, 13]             128\n",
      "              ReLU-7           [-1, 64, 13, 13]               0\n",
      "            Conv2d-8           [-1, 64, 13, 13]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 13, 13]             128\n",
      "             ReLU-10           [-1, 64, 13, 13]               0\n",
      "       BasicBlock-11           [-1, 64, 13, 13]               0\n",
      "           Conv2d-12           [-1, 64, 13, 13]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 13, 13]             128\n",
      "             ReLU-14           [-1, 64, 13, 13]               0\n",
      "           Conv2d-15           [-1, 64, 13, 13]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 13, 13]             128\n",
      "             ReLU-17           [-1, 64, 13, 13]               0\n",
      "       BasicBlock-18           [-1, 64, 13, 13]               0\n",
      "           Conv2d-19            [-1, 128, 7, 7]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 7, 7]             256\n",
      "             ReLU-21            [-1, 128, 7, 7]               0\n",
      "           Conv2d-22            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 7, 7]             256\n",
      "           Conv2d-24            [-1, 128, 7, 7]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 7, 7]             256\n",
      "             ReLU-26            [-1, 128, 7, 7]               0\n",
      "       BasicBlock-27            [-1, 128, 7, 7]               0\n",
      "           Conv2d-28            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 7, 7]             256\n",
      "             ReLU-30            [-1, 128, 7, 7]               0\n",
      "           Conv2d-31            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 7, 7]             256\n",
      "             ReLU-33            [-1, 128, 7, 7]               0\n",
      "       BasicBlock-34            [-1, 128, 7, 7]               0\n",
      "           Conv2d-35            [-1, 256, 4, 4]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 4, 4]             512\n",
      "             ReLU-37            [-1, 256, 4, 4]               0\n",
      "           Conv2d-38            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 4, 4]             512\n",
      "           Conv2d-40            [-1, 256, 4, 4]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 4, 4]             512\n",
      "             ReLU-42            [-1, 256, 4, 4]               0\n",
      "       BasicBlock-43            [-1, 256, 4, 4]               0\n",
      "           Conv2d-44            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 4, 4]             512\n",
      "             ReLU-46            [-1, 256, 4, 4]               0\n",
      "           Conv2d-47            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 4, 4]             512\n",
      "             ReLU-49            [-1, 256, 4, 4]               0\n",
      "       BasicBlock-50            [-1, 256, 4, 4]               0\n",
      "           Conv2d-51            [-1, 512, 2, 2]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-53            [-1, 512, 2, 2]               0\n",
      "           Conv2d-54            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 2, 2]           1,024\n",
      "           Conv2d-56            [-1, 512, 2, 2]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-58            [-1, 512, 2, 2]               0\n",
      "       BasicBlock-59            [-1, 512, 2, 2]               0\n",
      "           Conv2d-60            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-62            [-1, 512, 2, 2]               0\n",
      "           Conv2d-63            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-65            [-1, 512, 2, 2]               0\n",
      "       BasicBlock-66            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                    [-1, 5]           2,565\n",
      "================================================================\n",
      "Total params: 11,179,077\n",
      "Trainable params: 11,179,077\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 3.67\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 46.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summary(resnet18, input_size=(3, 50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50, 50])\n",
      "i = 0, Loss = 10.673742294311523\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 1, Loss = 11.761648178100586\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 2, Loss = 9.208263397216797\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 3, Loss = 7.706518173217773\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 4, Loss = 11.276969909667969\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 5, Loss = 11.177295684814453\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 6, Loss = 10.932086944580078\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 7, Loss = 10.251230239868164\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 8, Loss = 11.401750564575195\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 9, Loss = 11.730973243713379\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 10, Loss = 9.782238006591797\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 11, Loss = 10.7241849899292\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 12, Loss = 9.96330451965332\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 13, Loss = 10.999395370483398\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 14, Loss = 9.578664779663086\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 15, Loss = 10.976221084594727\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 16, Loss = 11.067859649658203\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 17, Loss = 10.346490859985352\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 18, Loss = 10.912240982055664\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 19, Loss = 9.581499099731445\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 20, Loss = 10.97852611541748\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 21, Loss = 11.53091812133789\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 22, Loss = 8.743422508239746\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 23, Loss = 10.7906494140625\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 24, Loss = 11.763992309570312\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 25, Loss = 10.65386962890625\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 26, Loss = 8.879081726074219\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 27, Loss = 11.065781593322754\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 28, Loss = 11.267194747924805\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 29, Loss = 11.218053817749023\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 30, Loss = 10.700325012207031\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 31, Loss = 11.396770477294922\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 32, Loss = 9.981654167175293\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 33, Loss = 10.178600311279297\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 34, Loss = 10.546329498291016\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 35, Loss = 12.100868225097656\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 36, Loss = 10.560064315795898\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 37, Loss = 9.716524124145508\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 38, Loss = 9.520766258239746\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 39, Loss = 12.995485305786133\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 40, Loss = 11.141424179077148\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 41, Loss = 11.195436477661133\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 42, Loss = 10.264270782470703\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 43, Loss = 9.63448715209961\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 44, Loss = 10.76899528503418\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 45, Loss = 10.035680770874023\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 46, Loss = 11.297773361206055\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 47, Loss = 11.561230659484863\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 48, Loss = 12.4178466796875\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 49, Loss = 11.084759712219238\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 50, Loss = 12.590476989746094\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 51, Loss = 11.644733428955078\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 52, Loss = 11.121658325195312\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 53, Loss = 10.481739044189453\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 54, Loss = 11.213407516479492\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 55, Loss = 10.403644561767578\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 56, Loss = 11.716670989990234\n",
      "torch.Size([2, 3, 50, 50])\n",
      "i = 57, Loss = 9.44368839263916\n",
      "torch.Size([2, 3, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/dj1311/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-6be1247d0080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs = data[0]\n",
    "    inp = torch.cat([data[0].mean(dim=-1).unsqueeze(1), data[0].mean(dim=-2).unsqueeze(1), data[0].mean(dim=-3).unsqueeze(1)], dim=1)\n",
    "    print(inp.shape)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = resnet18(inp)\n",
    "    loss = criterion(outputs, data[1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    print(f\"i = {i}, Loss = {loss.item()}\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
